{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m LabelEncoder\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msklearn\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model \u001b[39m=\u001b[39mNetwork()\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_diabetes\n\u001b[1;32m     10\u001b[0m X, y \u001b[39m=\u001b[39m load_diabetes(return_X_y\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'data'"
     ]
    }
   ],
   "source": [
    "from numpydl.model import Network,DenseLayer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sklearn\n",
    "\n",
    "model =Network()\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "\n",
    "def transform(x,y):\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "x,y=transform(X,y)\n",
    "\n",
    "\n",
    "model.add(DenseLayer(X.shape[0]))\n",
    "model.add(DenseLayer(8))\n",
    "model.add(DenseLayer(10))\n",
    "model.add(DenseLayer(3))\n",
    "\n",
    "model.train(X_train=x, y_train=y, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(low=-2, high=5, size=(6,4))\n",
    "ytrue = np.array([1,0,1,2,0,2])\n",
    "\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, neurons) -> None:\n",
    "        self.neurons = neurons\n",
    "        self.biases = np.zeros((1, neurons))\n",
    "        self.activation = None\n",
    "\n",
    "    def ReLU(self, inputs):\n",
    "        return np.maximum(0, inputs)\n",
    "    \n",
    "    def Softmax(self, inputs):\n",
    "        e_vals = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        return e_vals / np.sum(e_vals, axis=1, keepdims=True)\n",
    "    \n",
    "    def relu_derivative(self, dA, Z):\n",
    "        dZ = np.array(dA, copy = True)\n",
    "        dZ[Z <= 0] = 0;\n",
    "        return dZ;\n",
    "\n",
    "    def forward(self, inputs, last=False):\n",
    "        self.weights = np.random.uniform(low=-1, high=1, size=(inputs.shape[1], self.neurons))\n",
    "        if last == True:\n",
    "            self.Z = np.dot(inputs, self.weights) + self.biases\n",
    "            self.A = self.Softmax(self.Z)\n",
    "            self.activation = 'softmax'\n",
    "        else:\n",
    "            self.Z = np.dot(inputs, self.weights) + self.biases\n",
    "            self.A = self.ReLU(self.Z)\n",
    "            self.activation = 'relu'\n",
    "            \n",
    "    def backward(self, dA_curr, W_curr, Z_curr, A_prev):\n",
    "        m = A_prev.shape[1]\n",
    "\n",
    "        dZ_curr = self.relu_derivative(dA_curr, Z_curr)\n",
    "        dW_curr = np.dot(dZ_curr.T, A_prev) / m\n",
    "        db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "        dA_prev = np.dot(W_curr, dZ_curr.T)\n",
    "\n",
    "        return dA_prev, dW_curr, db_curr\n",
    "    \n",
    "\n",
    "class Network:\n",
    "    def __init__(self, data):\n",
    "        self.network = []\n",
    "        self.memory = {}\n",
    "        self.gradients = {}\n",
    "        self.data = data\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.network.append(layer)\n",
    "    \n",
    "    def _one_hot(self, labels):\n",
    "        ohy = np.zeros((labels.size, labels.max() + 1))\n",
    "        ohy[np.arange(labels.size), labels] = 1\n",
    "        ohy_t = ohy.T\n",
    "        return ohy_t\n",
    "        \n",
    "    def _calculate_loss(self, outputs, labels):\n",
    "        samples = len(labels)\n",
    "\n",
    "        out_clipped = np.clip(outputs, 1e-7, 1-1e-7)\n",
    "\n",
    "        if len(labels.shape) == 1:\n",
    "            confs = out_clipped[range(samples), labels]\n",
    "        elif len(labels.shape) == 2:\n",
    "            confs = np.sum(out_clipped*labels, axis=1)\n",
    "\n",
    "        return np.mean(-np.log(confs))\n",
    "    \n",
    "    def _backprop(self, actual_y, predict_y):\n",
    "        actual_y = self._one_hot(labels=actual_y)\n",
    "        actual_y = actual_y.reshape(predict_y.shape)\n",
    "        \n",
    "        dA_prev = - (np.divide(actual_y, predict_y) - np.divide(1 - actual_y, 1 - predict_y))\n",
    "        \n",
    "        for layer_idx_prev, layer in reversed(list(enumerate(self.network))):\n",
    "            layer_idx_curr = layer_idx_prev + 1\n",
    "            \n",
    "            if layer_idx_prev == 0:\n",
    "                A_prev = self.data\n",
    "            else:\n",
    "                ## if output layer --> insert dC/y_hat, otherwise update with dC/dZ\n",
    "                dA_curr = dA_prev\n",
    "\n",
    "                A_prev = self.memory[layer_idx_prev]['A']\n",
    "                Z_curr = self.memory[layer_idx_curr]['Z']\n",
    "                W_curr = self.memory[layer_idx_curr]['W']\n",
    "            \n",
    "            dA_prev, dW_curr, db_curr = layer.backward(dA_curr, W_curr, Z_curr, A_prev)\n",
    "            \n",
    "            self.gradients[layer_idx_curr] = {'dW':dW_curr, 'db':db_curr}\n",
    "    \n",
    "    def _forwardprop(self):\n",
    "        new_out = []\n",
    "        for idx, layer in enumerate(self.network):\n",
    "            if layer != self.network[-1]:\n",
    "                if not new_out:\n",
    "                    layer.forward(self.data)\n",
    "                    new_out.append(layer.A)\n",
    "                    self.memory[idx+1] = {'W':layer.weights, 'Z':layer.Z, 'A':layer.A,\n",
    "                                         'b':layer.biases}\n",
    "                else:\n",
    "                    layer.forward(new_out[-1])\n",
    "                    new_out.append(layer.A)\n",
    "                    self.memory[idx+1] = {'W':layer.weights, 'Z':layer.Z, 'A':layer.A,\n",
    "                                         'b':layer.biases}\n",
    "            else:\n",
    "                layer.forward(new_out[-1], last=True)\n",
    "                new_out.append(layer.A)\n",
    "                self.memory[idx+1] = {'W':layer.weights, 'Z':layer.Z, 'A':layer.A,\n",
    "                                      'b':layer.biases}\n",
    "        \n",
    "        return new_out[-1]\n",
    "    \n",
    "    def _update(self, lr=0.01):\n",
    "        for idx, layer in enumerate(self.network):\n",
    "            idx = idx + 1\n",
    "            self.memory[idx]['W'] -= learning_rate * self.gradients[idx]['dW']     \n",
    "            self.memory[idx]['b'] -= learning_rate * self.gradients[idx]['db']\n",
    "            \n",
    "    def _get_accuracy(self, predicted, actual):\n",
    "        return np.mean(np.argmax(predicted, axis=1)==actual)\n",
    "            \n",
    "\n",
    "model = Network(data=X)\n",
    "model.add(DenseLayer(neurons=X.shape[0]))\n",
    "model.add(DenseLayer(neurons=6))\n",
    "model.add(DenseLayer(neurons=3))\n",
    "out = model._forwardprop()\n",
    "\n",
    "model._backprop(actual_y=ytrue, predict_y=out)\n",
    "\n",
    "# loss = model.calculate_loss(outputs=out, labels=ytrue)\n",
    "\n",
    "# print('PREDICTIONS:', np.argmax(out, axis=1))\n",
    "# print('ACTUAL:', ytrue)\n",
    "# print('LOSS:', loss)\n",
    "# print('ACCURACY:', np.mean(np.argmax(out, axis=1)==ytrue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTIONS: [0 0 0 0 0 0]\n",
      "ACTUAL: [1 0 1 2 0 2]\n",
      "LOSS: 2.953763258510348\n",
      "ACCURACY: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "loss = model._calculate_loss(outputs=out, labels=ytrue)\n",
    "print('PREDICTIONS:', np.argmax(out, axis=1))\n",
    "print('ACTUAL:', ytrue)\n",
    "print('LOSS:', loss)\n",
    "print('ACCURACY:', np.mean(np.argmax(out, axis=1)==ytrue))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f06f4baf162c1bba868b3ed890ca059fd0c13f705dc9d48413e36c65b86dd60d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
